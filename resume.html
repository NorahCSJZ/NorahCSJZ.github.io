<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>AMOS T</title><meta name="author" content="Rob"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; margin:0pt; }
 .h2, h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10.5pt; }
 .s1 { color: black; font-family:TimesNewRomanPS-BoldItalicMT, serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 10.5pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s3 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .h3, h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9.5pt; }
 .s4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 li {display: block; }
 #l1 {padding-left: 0pt; }
 #l1> li>*:first-child:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; }
 li {display: block; }
 #l2 {padding-left: 0pt; }
 #l2> li>*:first-child:before {content: "• "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; }
 li {display: block; }
 #l3 {padding-left: 0pt; }
 #l3> li>*:first-child:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><span><img width="745" height="1" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_001.png"/></span></p><h1 style="padding-left: 234pt;text-indent: 0pt;line-height: 13pt;text-align: center;">JINGZHOU SHEN</h1><p style="padding-left: 122pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="mailto:jingzhou1127shen@163.com" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt;" target="_blank">•+1 202 679 7740 •</a><span style=" color: #00F; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt;">jingzhou1127shen@163.com</span>•homepage: norahcsjz.github.io</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="745" height="2" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_002.png"/></span></p><h1 style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark0">EDUCATIONAL BACKGROUND</a></h1><h2 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">08/2021-05/2023 The George Washington University                              <span class="p">Washington, US</span></h2><h2 style="padding-top: 4pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">Major<span class="p">: Computer Science; </span>Degree<span class="p">: Master of Science; </span>GPA<span class="p">: 4.0/4.0</span></h2><p class="s1" style="padding-top: 4pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">George Washington University Student Scholarship (TOP 1% )</p><h2 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">09/2017-06/2021 Huaqiao University                                         <span class="p">Xiamen, CN</span></h2><h2 style="padding-top: 4pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">Major<span class="p">: Computer Science and Technology; </span>Degree<span class="p">: Bachelor of Science; </span>GPA<span class="p">: 3.414/5.0</span></h2><p class="s1" style="padding-top: 2pt;padding-left: 81pt;text-indent: 0pt;text-align: left;">Third Prize Scholarship<span class="h2">(Top 30%)</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="749" height="2" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_003.png"/></span></p><h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark1">RESEARCHES</a></h1><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">06/2022-Present Time-flow Prediction with ViT Variants                           <span class="p">Washington, US</span></h2><p class="s2" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Researcher, Supervised by Professor Robert Pless</p><ul id="l1"><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Used Resnet/ViT to predict the time for one steady camera and for the trained model;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Visualized the attention of the model by using GradCAM and heatmap;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Predicted the time information in the picture under a given camera with the improved ViT;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Extracted q and k for each batch, multiplied them together and showed them in the heatmap to see how the time changes;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Tracked how the model detected time through the ViT and visualized temporal information within the probe model;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 12pt;text-indent: -5pt;text-align: left;">Made oral representation at AIPR(IEEE workshop).</p><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">05/2022-Present Robust Deep Graph Learning for Dynamic Graphs                     <span class="p">Washington, US</span></h2><p class="s2" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Researcher, Supervised by Professor Luosheng Dong</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 12pt;text-indent: -5pt;line-height: 124%;text-align: left;">Extracted the features from datasets of the brain network, DBLP, Epinion, Reddit , mdke adjacency matrix for each time step, and preprocessed the dynamic Graph;</p></li><li data-list-text="•"><p style="padding-left: 13pt;text-indent: -6pt;text-align: left;">Used GNNs for dynamic Graph and Graph Structure Learning to get a better and clean graph;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 124%;text-align: left;">Improved the performance of dynamic Graph Neural Network, its denoising part with AdaNN and the denoise method in PTDNet. <b>05/2022-Present Multimodal Graph NN in Brain Network                           </b>Washington, US <i>Researcher, Supervised by Professor Hongchang Gao</i></p></li><li data-list-text="•"><p style="padding-left: 12pt;text-indent: -5pt;line-height: 124%;text-align: left;">Used multimodal Graph NN on the different representations of brain network based on the two brain networks datasets: DTInetwork and fMRI network;</p></li><li data-list-text="•"><p style="padding-left: 13pt;text-indent: -6pt;text-align: left;">Combined the learned features of both sides, and used MLP to extract the features together;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 124%;text-align: left;">Designed a unique loss function to gather information from the above two brain networks, and iterated each feature to get a new model.</p><h2 style="padding-left: 7pt;text-indent: 0pt;text-align: left;">01/2022-06/2022 Self-Supervised Learning Movement and Its Physics Law                  <span class="p">Washington, US</span></h2><p class="s2" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Researcher, Supervised by Professor Rui Liu</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Simulated the scenes of robots with MuJuCo;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 12pt;text-indent: -5pt;line-height: 124%;text-align: left;">Analyzed what parameters are needed in the model and what to be expected from the model under the condition of the four scenarios of robots grasping the target,</p></li><li data-list-text="•"><p style="padding-left: 13pt;text-indent: -6pt;text-align: left;">Utilized self-supervised datasets to learn the physics law through a robot (based on PINN and SymbolicGPT);</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Implentmented PINN + SymbolicGPT into a robot to evaluate its movement mode and the physics law behind it.</p><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">08/2021-02/2022 MOSI-Hate speech Project                                   <span class="p">Washington, US</span></h2><p class="s2" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Supervised by Professor Robert Pless</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Crawled the information from social media like Reddit, Twitter, and other platforms;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Defined several types of the images like sexuality, violence and etc.,</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Conducted image processing and effectively analyzed images with obvious negative emotions from social networks;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Classified images by Vision Transformer and detected the emotion of the words in the images by Bert;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Put the above information into ALBEF and CLIP to train the mode,l, and predicted the emotion of a new image.</p><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">01/2020-06/2020 Image Processing of Fundus Lesions Based on Deep Learning                 <span class="p">Xiamen, CN</span></h2><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="745" height="1" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_004.png"/></span></p><p class="s2" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Supervised by Professor Jialin Peng</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Used public dataset diaretdb as the main data source;</p></li><li data-list-text="•"><p style="padding-top: 1pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Recognized four types of diseases in the fundus image, among which each one has some particular areas/features in the image<span class="s3">；</span></p></li><li data-list-text="•"><p style="padding-left: 13pt;text-indent: -6pt;text-align: left;">Analyzed the fundus image pathology by using U-Net, U2Net first to test the performance<span class="s3">；</span></p></li><li data-list-text="•"><p style="padding-left: 13pt;text-indent: -6pt;text-align: left;">Created U2-Net + PSPNET to test the performance in the datasets<span class="s3">；</span></p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Constructed a neural network that could analyze the pathological images of the fundus.</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="745" height="2" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_005.png"/></span></p><h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark2">INDIVIDUAL PROJECTS</a></h1><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">02/2022-05/2022 Face Detection Based on YoloV5, with Kkey Point Detection                <span class="p">Washington, US</span></h2><ul id="l2"><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Added the key point regression branch based on YoloV5 detection and solved outliers of key points;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Realized to frame a face model in an image, even if the face was small.</p><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">02/2022-05/2022 Image Morphing, <i>Supervised by Professor Robert Pless                    </i><span class="p">Washington, US</span></h2></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Morphed images into one with Python and Sklearn;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 12pt;text-indent: -5pt;text-align: left;">Used SIFT and kernel methods based on the Opencv package to achieve some goals in morphing.</p><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">09/2021-12/2021 Game Design, <i>Supervised by Professor Juman Byun                      </i><span class="p">Washington, US</span></h2></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Developed a pass-through FPS game with pygame and Gamemaker Studio.</p><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">08/2021-12/2021 Realization of Gender and Age Prediction of Human Face based on RESNET       <span class="p">Washington, US</span></h2></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Utilized RESNET 50 to correctly acquire the gender and age from human face images.</p></li></ul><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">09/2019-01/2020 Analysis of the Changes in Women&#39;s Rights                           <span class="p">Xiamen, CN</span></h2><p class="s2" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Team Leader, Supervised by Dr. Huixin He</p><ul id="l3"><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Obtained related data with Python scrapy packages(like beautifulsoup), set up Oracle database, and maintained the data;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Analyzed the data construction model to get real reasons for women&#39;s rights changes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="745" height="2" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_006.png"/></span></p><h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark3">INTERNSHIPS</a></h1><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">08/2022-Present <i>Student Specialist, </i>GWIT, George Washington University                  <span class="p">Washington, US</span></h2></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Maintained George Washington University IT-related work.</p></li><li data-list-text="•"><h2 style="padding-top: 2pt;padding-left: 11pt;text-indent: -3pt;text-align: left;">03/2021-07/2021 <i>Algorithm Engineer, AI Institute, </i>Beijing Zhipu Huazhang Technology Co., Ltd.        <span class="p">Beijing, CN</span></h2><h3 style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Project: Intelligent Voice Question and Answer Robot ( for Xiaomai The Beijing Science and Technology Exhibition)</h3></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 12pt;text-indent: -5pt;line-height: 124%;text-align: left;">Assisted algorithm engineer in developing and testing algorithm with neural network model for speech recognition, question and answer polymeric neural network model, map navigation, and position function, part of the algorithm;</p></li><li data-list-text="•"><p style="padding-left: 12pt;text-indent: -5pt;text-align: left;">Optimized the FAQ system in the robot with Deep Neural Network and GBDT;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 12pt;text-indent: -5pt;text-align: left;">Responsible for data mining information on COVID-19 and other basic knowledge for the robot FAQ system;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Reproduced DSSM and other question-and-answer polymerization models;</p></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -5pt;text-align: left;">Applied the API of Feng Map and Amap to realize the positioning and navigation functions of the robot.</p><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">08/2020-10/2020 <i>Network Engineer, Network Department, </i>Xiamen Net Box Technology Co., Ltd         <span class="p">Xiamen, CN</span></h2></li><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 13pt;text-indent: -6pt;text-align: left;">Focused on the design of integrated cabling scheme and enterprise network security and set up network topology in the work area.</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="745" height="2" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_007.png"/></span></p><h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark4">TEACHING EXPERIENCE</a></h1><p class="s2" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><b>08/2022-Present </b>Teaching Assistant<span class="h3">, Discrete Structure </span><span class="p">by Professor Hyeong-Ah Choi, </span>20 hours/week      <span class="p">Washington, US</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="745" height="2" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_008.png"/></span></p><h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark5">REVIEWER OF CONFERENCE PAPERS</a></h1><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">11/2022 <i>Deep Contour Attention Learning for Scleral Deformation from OCT Images, </i><span class="p">IEEE TNNLS</span></h2><h2 style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">08/2022 <i>Robust Graph Embedding via Attack-aid Graph Denoising, </i><span class="p">ICDM</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="745" height="2" alt="image" src="Jingzhou%20Shen%20Resume_files/Image_009.png"/></span></p><h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark6">COMPUTER SKILLS</a></h1><p class="s4" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">C++, JAVA, Python, Pytorch, Keras, Sklearn, Beautifulsoup, Pyplot, Numpy, Matplotlib, PIL, Hadoop, MongoDB etc.</p></body></html>
